---
layout:     post
title:      机器学习-蒙特卡洛搜索树
subtitle:   智能黑白棋
date:       2020-03-20
author:     Stitch
header-img: img/post-bg-debug.png
catalog: true
tags:
    - Algorithm
    - Machine Learning
--- 

## 问题引入 ##

蒙特卡洛树搜索（MCTS）是一种在人工智能问题中进行决策优化的方法，通常是对于那些在组合游戏中需要移动规划的部分。传统的树搜索策略往往是遍历部分搜索空间，然后结合剪枝或者启发式函数减少剩余搜索空间规模。蒙特卡洛树搜索将随机模拟的通用性与树搜索的准确性进行了结合。在2016-2017年，大名鼎鼎的Alpha-Go便是凭借蒙特卡洛搜索和神经网络击败了世界各大顶级围棋手。蒙特卡洛搜索起源于围棋中的移动规划，但如今它的应用领域已不止于博弈，而且理论上 MCTS 可以应用于任何能够以 {状态，动作} 形式描述，通过模拟来预测结果的领域。

## 算法描述 ##

蒙特卡洛搜索树的结构与普通的搜索树并无二异，树的根结点代表当前状态，子结点代表后续状态，如围棋的棋盘状态；父节点到子节点的转移代表动作，如行棋。蒙特卡洛搜索与普通搜索的最大不同在于，蒙特卡洛搜索决定下一步动作是基于对后续状态大量的采样和模拟。我们知道，根据大数定理，采样次数越多，事件的频率越接近真实概率。那么只要我们采样次数足够多，模拟执行每一步后续动作，我们就能知道每一步后续动作的获胜概率，其中胜率最高的动作无疑就是我们所需要的。

蒙特卡洛搜索树算法可以分解为以下四步：

1. 选择。从根节点开始，递归地选择最优子节点（最有希望获胜的结点），直至到达一个叶节点，并选择这个叶节点。
```python
def select(self, node: Node) -> Node:
    #找到一个最优的叶节点
    res = node
    while len(res.children) > 0:
        res = self.findBestNode(res)
    return res
```
2. 拓展。为选择的叶节点拓展子节点。换句话说，将该叶节点的后续状态添加到搜索树上。
```python
def expand(self, leafNode: Node):
    # 拓展叶节点(添加其子节点)
    for action in leafNode.state.getPossibleActions():
        board = deepcopy(leafNode.state.getBoard())
        board._move(action, leafNode.state.player)
        newNode = Node(board, leafNode.state.getOpponent())
        newNode.state.action = action
        leafNode.children.append(newNode)
```
3. 模拟。从选择的叶节点开始按一定策略进行一次模拟对局，直至达到终止状态。例如以快速走子策略行棋，双方每次都从自己的所有可行动作中随机挑选一个作为自己的下一步动作，双方交替模拟行棋，直至一方获胜。
```python
def rollout(self, nodeToExplore: Node) -> str:
    # 按照"快速行棋"的原则进行一次模拟,直至游戏结束
    state = deepcopy(nodeToExplore.state)
    boardStatus = state.isGameOver()
    while not boardStatus:
        state.switchPlayer()
        state.randomPlay()
        boardStatus = state.isGameOver()
    winner = state.getBoard().get_winner()
    if winner == 0:
        return "X"
    elif winner == 1:
        return "O"
    else:
        return ""
```
4. 反馈。将模拟结果从选择的叶节点开始一步步向后传导直至根节点，并更新沿途各节点的参数（基于模拟结果的价值估计，被访问的次数）。
```python
def backPropogation(self, nodeToExplore: Node, winner: str):
    # 反向传播,更新叶节点到根节点路径上的访问次数和收益
    node = nodeToExplore
    while node != None:
        node.state.visitCount += 1
        node.state.winScore += self.__winScore if node.state.player == winner else 0
        node = node.parent
```

重复上述步骤，直至达到预设的采样次数或耗尽采样时间。根节点下的最优节点即是最优的后续状态。

![UCB公式](https://raw.githubusercontent.com/StitchWuhula/StitchWuhula.github.io/master/img/2020-03-20/1.png)

## 算法说明 ##

### 最优子节点 ###

我们在前面为了方便，用胜率来描述一个节点的价值估计，但这样说其实是不准确的。我们知道蒙特卡洛搜索中的胜负是基于随机模拟的，所以节点在变的可靠之前必须要被访问一定的次数。因此我们在选择节点时，不仅要考虑该节点基于模拟结果的价值估计，也应当将访问次数考虑在内。上限置信区间（Upper Confidence Bounds, UCB）值是节点价值的一个更加真实和可靠的表示。

![UCB公式](https://raw.githubusercontent.com/StitchWuhula/StitchWuhula.github.io/master/img/2020-03-20/2.png)

其中 vi 是节点基于模拟结果的估值，ni 是节点被访问的次数而 N 是它的父亲节点被访问的总次数。C 是可调的偏置参数。

从公式中不难看出，即使节点本身未被访问，其UCB值也会随着兄弟节点的访问而增加。这意味着，采用UCB公式不仅鼓励访问胜率较大（公式第一项）的节点，也鼓励访问那些访问次数较少（公式第二项）的节点。这样的做法在利用性和探索性之间做了较好的平衡，能更加充分地反映每个节点的真实价值。

### 算法的优点 ###

蒙特卡洛搜索树的一大优点是无需借助任何先验策略，也无需知道关于该博弈的先验知识（除了终止条件）。因此，不同棋类之间只需修改少量代码即可实现移植。

此外，该算法可以在任意时间中止并返回当前最佳的评估策略。建立的搜索树可以被抛弃或为以后的复用而保留。

易于实现或许是其流行的另一个重要原因。